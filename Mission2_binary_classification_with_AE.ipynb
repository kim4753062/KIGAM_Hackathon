{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification for identifying abnormal instances"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "***\n",
    "**Contest:** 2023년 지질자원 데이터 활용 및 인공지능 경진대회 (link [here](http://www.geodata-con.kr/2023/overview.php)).  \n",
    "**Original:** <i><b>A Realistic and Public Dataset with Rare Undesirable Real Events in Oil Wells</i></b> published in the <i><b>Journal of Petroleum Science and Engineering</i></b> (link [here](https://doi.org/10.1016/j.petrol.2019.106223)).  \n",
    "**Editor:** Jongwook Kim, Dogyun Kim  \n",
    "**Advisor:** Jonggeun Choe  \n",
    "**Last updated:** 08-16-2023\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports and Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "\n",
    "import matplotlib.dates\n",
    "import numpy as np\n",
    "import torch.cuda\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "from module import *\n",
    "from argparse import ArgumentParser\n",
    "from models import *\n",
    "from copy import copy\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "data_path = Path('data')\n",
    "fix_seed()\n",
    "events_names = {0: 'Normal',\n",
    "                1: 'Abrupt Increase of BSW',\n",
    "                2: 'Spurious Closure of DHSV',\n",
    "                3: 'Severe Slugging',\n",
    "                4: 'Flow Instability',\n",
    "                5: 'Rapid Productivity Loss',\n",
    "                6: 'Quick Restriction in PCK',\n",
    "                7: 'Scaling in PCK',\n",
    "                8: 'Hydrate in Production Line'\n",
    "               }\n",
    "vars = ['P-PDG',\n",
    "        'P-TPT',\n",
    "        'T-TPT',\n",
    "        'P-MON-CKP',\n",
    "        'T-JUS-CKP',\n",
    "        'P-JUS-CKGL',\n",
    "        'T-JUS-CKGL',\n",
    "        'QGL']\n",
    "\n",
    "#parsing the parameters\n",
    "args = ArgumentParser()\n",
    "args.columns = ['timestamp'] + vars + ['class']\n",
    "args.abnormal_classes_codes = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "args.downsample_rate = 30            # Adjusts frequency of sampling to the dynamics\n",
    "                                # of the undesirable event of interest\n",
    "args.sample_size_default = 60        # In observations (after downsample)\n",
    "args.sample_size_normal_period = 5   # In observations (after downsample)\n",
    "args.max_nan_percent = 0.15           # For selection of useful variables\n",
    "args.std_vars_min = 0.01             # For selection of useful variables\n",
    "args.max_frozen_percent = 0.1\n",
    "args.disable_progressbar = True      # For less output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T10:15:45.002203900Z",
     "start_time": "2023-08-15T10:15:41.533382700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of total instances = 1984\n",
      "number of real instances = 1025\n",
      "number of simulated instances = 939\n",
      "number of drawn instances = 20\n",
      "\n",
      "Clean Variables: ['P-TPT', 'T-TPT', 'P-MON-CKP']\n"
     ]
    }
   ],
   "source": [
    "df_instances_clean, clean_vars, instances, real_instances, sim_instances, drawn_instances = Preprocessing(args, vars, data_path)\n",
    "df = copy(df_instances_clean)\n",
    "df.loc[df['source']=='drawn', ['P-TPT', 'P-MON-CKP']] = df.loc[df['source']=='drawn', ['P-TPT', 'P-MON-CKP']] * 1e5\n",
    "new_instances = copy(instances)\n",
    "\n",
    "df, new_instances = make_augmentation(df, new_instances, real_instances, sim_instances, drawn_instances)\n",
    "add_instances = new_instances[1984:]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T10:18:04.363042700Z",
     "start_time": "2023-08-15T10:15:45.003204200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T10:18:10.025354Z",
     "start_time": "2023-08-15T10:18:07.913881500Z"
    }
   },
   "outputs": [],
   "source": [
    "df_real = df_instances_clean[df_instances_clean['source']=='real']\n",
    "# X (real)\n",
    "real_id = list(set(df_real['instance_id']))\n",
    "# Y (real)\n",
    "real_class= list(instances.loc[real_id]['class_code'])\n",
    "\n",
    "df_sim = df_instances_clean[df_instances_clean['source']=='simulated']\n",
    "sim_id = list(set(df_sim['instance_id']))\n",
    "sim_class= list(instances.loc[sim_id]['class_code'])\n",
    "\n",
    "\n",
    "df_drawn = df_instances_clean[df_instances_clean['source']=='drawn']\n",
    "drawn_id = list(set(df_drawn['instance_id']))\n",
    "drawn_class= list(instances.loc[drawn_id]['class_code'])\n",
    "\n",
    "df_synthetic = pd.concat([df_sim, df_drawn], axis=0).reset_index(drop=True)\n",
    "synthetic_id = sim_id + drawn_id\n",
    "synthetic_class = sim_class + drawn_class"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. LSTM Autoencoder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "args.train_ratio = 0.6\n",
    "args.validation_ratio = 0.2\n",
    "args.test_ratio = 0.2\n",
    "RF = False\n",
    "by_instance = True\n",
    "use_synthetic = True\n",
    "use_augmentation = True\n",
    "integrate = False\n",
    "integrate_syn = False\n",
    "\n",
    "# Real_Only | With_Sim_Drawn | With_Sim_Drawn_Augmentation\n",
    "# 첫번째 Each => scaler를 real이랑 synthetic이랑 분리\n",
    "# 두번째 Each => scaler를 synthetic 내에서 simulatd랑 drawn이랑 분리\n",
    "# Byinstance => instance마다 스케일링\n",
    "name = 'Byinstance_With_Sim_Drawn_Augmentation_multi'\n",
    "if not 'fig' in os.listdir():\n",
    "    os.mkdir('fig')\n",
    "if not 'Mission2' in os.listdir('fig'):\n",
    "    os.mkdir('./fig/Mission2')\n",
    "    for event in events_names:\n",
    "        if not str(int(event)) in os.listdir('./fig/Mission2'):\n",
    "            os.mkdir(f'./fig/Mission2/{int(event)}')\n",
    "\n",
    "# 모델 및 학습 파라미터 설정\n",
    "param = ArgumentParser()\n",
    "param.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "param.num_epoch = 100\n",
    "param.batch_size = 64\n",
    "param.learning_rate = 0.003\n",
    "param.window_size = 30\n",
    "param.stride = 15\n",
    "\n",
    "param.input_dim = len(clean_vars)\n",
    "param.n_features = len(events_names)\n",
    "param.hidden_dim = 128\n",
    "param.num_layers = 3\n",
    "param.vicinal_risk_minimization = False\n",
    "param.masking = False\n",
    "param.mask_ratio = 0.9\n",
    "param.batch_max_mask_ratio = 0.25\n",
    "param.sim_use = False\n",
    "param.drawn_use = False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T10:46:34.993474800Z",
     "start_time": "2023-08-15T10:46:34.872434400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stratified sampling & Scaling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ratio: 0.68\n",
      "Validation ratio: 0.23\n",
      "Test ratio: 0.09\n"
     ]
    }
   ],
   "source": [
    "fix_seed()\n",
    "train_id, valid_id, test_id = [], [], []\n",
    "real_idx_dict = {}\n",
    "train_idx_dict = {}\n",
    "valid_idx_dict = {}\n",
    "test_idx_dict = {}\n",
    "\n",
    "for class_code in set(real_instances['class_code']):\n",
    "    real_idx_dict[class_code] = list(real_instances[real_instances['class_code'] == class_code].index.values)\n",
    "\n",
    "for class_code in set(real_instances['class_code']):\n",
    "    # 2023-08-11: Compatible with Simulated and Drawn dataset\n",
    "    if ceil(args.test_ratio * len(real_instances[real_instances['class_code'] == class_code])) <= 1:\n",
    "        class_tmp = [int(real_idx_dict[class_code].pop(np.random.randint(len(real_idx_dict[class_code]))))]\n",
    "        test_id += class_tmp\n",
    "        train_valid_id = real_idx_dict[class_code]\n",
    "        if use_synthetic:\n",
    "            train_valid_id += list(sim_instances[sim_instances['class_code'] == class_code].index.values + 1025)\n",
    "            train_valid_id += list(drawn_instances[drawn_instances['class_code'] == class_code].index.values + 1964)\n",
    "        if use_augmentation:\n",
    "            train_valid_id += list(add_instances[add_instances['class_code'] == class_code].index.values)\n",
    "        train_id_class, valid_id_class = train_test_split(train_valid_id, test_size=args.validation_ratio/(1-args.test_ratio))\n",
    "        train_id += train_id_class\n",
    "        valid_id += valid_id_class\n",
    "\n",
    "        train_idx_dict[class_code] = train_id_class\n",
    "        valid_idx_dict[class_code] = valid_id_class\n",
    "        test_idx_dict[class_code] = class_tmp\n",
    "    else:\n",
    "        test_id_class = np.random.choice(real_idx_dict[class_code], size=ceil(args.test_ratio * len(real_instances[real_instances['class_code'] == class_code])), replace=False)\n",
    "        test_id += list(test_id_class)\n",
    "        train_valid_id = list(set(real_idx_dict[class_code]) - set(test_id_class))\n",
    "        if use_synthetic:\n",
    "            train_valid_id += list(sim_instances[sim_instances['class_code'] == class_code].index.values + 1025)\n",
    "            train_valid_id += list(drawn_instances[drawn_instances['class_code'] == class_code].index.values + 1964)\n",
    "        if use_augmentation:\n",
    "            train_valid_id += list(add_instances[add_instances['class_code'] == class_code].index.values)\n",
    "        train_id_class, valid_id_class = train_test_split(train_valid_id, test_size=args.validation_ratio/(1-args.test_ratio))\n",
    "        train_id += train_id_class\n",
    "        valid_id += valid_id_class\n",
    "\n",
    "        train_idx_dict[class_code] = train_id_class\n",
    "        valid_idx_dict[class_code] = valid_id_class\n",
    "        test_idx_dict[class_code] = list(test_id_class)\n",
    "if RF:\n",
    "    train_id = sorted(train_id + valid_id)\n",
    "    test_id = sorted(test_id)\n",
    "\n",
    "    print(f'Train ratio: {len(train_id) / len(train_id+test_id):.2f}')\n",
    "    print(f'Test ratio: {len(test_id) / len(train_id+test_id):.2f}')\n",
    "else:\n",
    "    train_id = sorted(train_id)\n",
    "    valid_id = sorted(valid_id)\n",
    "    test_id = sorted(test_id)\n",
    "\n",
    "    print(f'Train ratio: {len(train_id) / len(train_id+valid_id+test_id):.2f}')\n",
    "    print(f'Validation ratio: {len(valid_id) / len(train_id+valid_id+test_id):.2f}')\n",
    "    print(f'Test ratio: {len(test_id) / len(train_id+valid_id+test_id):.2f}')\n",
    "\n",
    "\n",
    "df_train = make_data(df, train_id)\n",
    "df_valid = make_data(df, valid_id)\n",
    "df_test = make_data(df, test_id)\n",
    "\n",
    "if by_instance:\n",
    "    df_train = fit_scaler(train_id, df_train, clean_vars)\n",
    "    df_valid = fit_scaler(valid_id, df_valid, clean_vars)\n",
    "    df_test = fit_scaler(test_id, df_test, clean_vars)\n",
    "\n",
    "else:\n",
    "    if integrate:\n",
    "        scaler_train = make_scaler(train_id, df_train, clean_vars)\n",
    "        df_train = fit_scaler_old(train_id, df_train, clean_vars, scaler_train)\n",
    "        df_valid = fit_scaler_old(valid_id, df_valid, clean_vars, scaler_train)\n",
    "        df_test = fit_scaler_old(test_id, df_test, clean_vars, scaler_train)\n",
    "    else:\n",
    "        train_id = np.array(train_id)\n",
    "        train_id_real = train_id[train_id < 1025]\n",
    "        scaler_train_real = make_scaler(train_id_real, df_train, clean_vars)\n",
    "        df_train = fit_scaler_old(train_id_real, df_train, clean_vars, scaler_train_real)\n",
    "        df_valid = fit_scaler_old(valid_id, df_valid, clean_vars, scaler_train_real)\n",
    "        df_test = fit_scaler_old(test_id, df_test, clean_vars, scaler_train_real)\n",
    "\n",
    "        if integrate_syn:\n",
    "            train_id_syn = train_id[train_id >= 1025]\n",
    "            valid_id_syn = valid_id[valid_id >= 1025]\n",
    "            scaler_train_syn = make_scaler(train_id_syn, df_train, clean_vars)\n",
    "            df_train = fit_scaler_old(train_id_syn, df_train, clean_vars, scaler_train_syn)\n",
    "            df_train = fit_scaler_old(valid_id_syn, df_train, clean_vars, scaler_train_syn)\n",
    "        else:\n",
    "            train_id_sim = set(df_train.loc[df_train['source']=='simulated', 'instance_id'])\n",
    "            train_id_drawn = set(df_train.loc[df_train['source']=='drawn', 'instance_id'])\n",
    "            valid_id_sim = set(df_valid.loc[df_valid['source']=='simulated', 'instance_id'])\n",
    "            valid_id_drawn = set(df_valid.loc[df_valid['source']=='drawn', 'instance_id'])\n",
    "\n",
    "            scaler_train_sim = make_scaler(train_id_sim, df_train, clean_vars)\n",
    "            scaler_train_drawn = make_scaler(train_id_drawn, df_train, clean_vars)\n",
    "\n",
    "            df_train = fit_scaler_old(train_id_sim, df_train, clean_vars, scaler_train_sim)\n",
    "            df_train = fit_scaler_old(train_id_drawn, df_train, clean_vars, scaler_train_drawn)\n",
    "            df_valid = fit_scaler_old(valid_id_sim, df_valid, clean_vars, scaler_train_sim)\n",
    "            df_valid = fit_scaler_old(valid_id_drawn, df_valid, clean_vars, scaler_train_drawn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "train_dataset = ByInstanceDataset(data=df_train,instance_id=train_id, all_instances=new_instances, input_vars=clean_vars,\n",
    "                                          stats = None, is_pretrain=True,\n",
    "                                          scaler=None, already_scaled=True)\n",
    "valid_dataset = ByInstanceDataset(data=df_valid, instance_id=valid_id, all_instances=new_instances, input_vars=clean_vars,\n",
    "                                  stats = None, is_pretrain=True,\n",
    "                                  scaler=None, already_scaled=True)\n",
    "test_dataset = ByInstanceDataset(data=df_test, instance_id=test_id, all_instances=new_instances, input_vars=clean_vars,\n",
    "                                 stats = None, is_pretrain=True,\n",
    "                                 scaler=None, already_scaled=True)\n",
    "\n",
    "dataloader_dict = {}\n",
    "dataloader_dict['Train'] = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "dataloader_dict['Valid'] = DataLoader(valid_dataset, batch_size=1, shuffle=True)\n",
    "dataloader_dict['Test'] = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fix_seed()\n",
    "model = LSTMClassifier(input_dim=param.input_dim,\n",
    "                       n_features=param.n_features,\n",
    "                       window_size=param.window_size,\n",
    "                       latent_dim=param.hidden_dim,\n",
    "                       device=param.device,\n",
    "                       num_layers=param.num_layers)\n",
    "criterion = nn.MSELoss()\n",
    "params_to_optimize = [{'params': model.encoder.parameters()}, {'params': model.reconstruct_decoder.parameters()}]\n",
    "optimizer = torch.optim.Adam(params_to_optimize, lr=param.learning_rate)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "\n",
    "model, logger = Pretraining(param, model, criterion, optimizer, scheduler, dataloader_dict, model_name=f'{name}_autoencoder')\n",
    "draw_loss(logger,fname=f'{name}_autoencoder')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Data selection using LSTM Autoencoder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "by_instance = True\n",
    "use_synthetic = False\n",
    "use_augmentation = False\n",
    "integrate = False\n",
    "integrate_syn = False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ratio: 0.59\n",
      "Validation ratio: 0.20\n",
      "Test ratio: 0.20\n"
     ]
    }
   ],
   "source": [
    "fix_seed()\n",
    "train_id, valid_id, test_id = [], [], []\n",
    "real_idx_dict = {}\n",
    "train_idx_dict = {}\n",
    "valid_idx_dict = {}\n",
    "test_idx_dict = {}\n",
    "\n",
    "for class_code in set(real_instances['class_code']):\n",
    "    real_idx_dict[class_code] = list(real_instances[real_instances['class_code'] == class_code].index.values)\n",
    "\n",
    "for class_code in set(real_instances['class_code']):\n",
    "    # 2023-08-11: Compatible with Simulated and Drawn dataset\n",
    "    if ceil(args.test_ratio * len(real_instances[real_instances['class_code'] == class_code])) <= 1:\n",
    "        class_tmp = [int(real_idx_dict[class_code].pop(np.random.randint(len(real_idx_dict[class_code]))))]\n",
    "        test_id += class_tmp\n",
    "        train_valid_id = real_idx_dict[class_code]\n",
    "        if use_synthetic:\n",
    "            train_valid_id += list(sim_instances[sim_instances['class_code'] == class_code].index.values + 1025)\n",
    "            train_valid_id += list(drawn_instances[drawn_instances['class_code'] == class_code].index.values + 1964)\n",
    "        if use_augmentation:\n",
    "            train_valid_id += list(add_instances[add_instances['class_code'] == class_code].index.values)\n",
    "        train_id_class, valid_id_class = train_test_split(train_valid_id, test_size=args.validation_ratio/(1-args.test_ratio))\n",
    "        train_id += train_id_class\n",
    "        valid_id += valid_id_class\n",
    "\n",
    "        train_idx_dict[class_code] = train_id_class\n",
    "        valid_idx_dict[class_code] = valid_id_class\n",
    "        test_idx_dict[class_code] = class_tmp\n",
    "    else:\n",
    "        test_id_class = np.random.choice(real_idx_dict[class_code], size=ceil(args.test_ratio * len(real_instances[real_instances['class_code'] == class_code])), replace=False)\n",
    "        test_id += list(test_id_class)\n",
    "        train_valid_id = list(set(real_idx_dict[class_code]) - set(test_id_class))\n",
    "        if use_synthetic:\n",
    "            train_valid_id += list(sim_instances[sim_instances['class_code'] == class_code].index.values + 1025)\n",
    "            train_valid_id += list(drawn_instances[drawn_instances['class_code'] == class_code].index.values + 1964)\n",
    "        if use_augmentation:\n",
    "            train_valid_id += list(add_instances[add_instances['class_code'] == class_code].index.values)\n",
    "        train_id_class, valid_id_class = train_test_split(train_valid_id, test_size=args.validation_ratio/(1-args.test_ratio))\n",
    "        train_id += train_id_class\n",
    "        valid_id += valid_id_class\n",
    "\n",
    "        train_idx_dict[class_code] = train_id_class\n",
    "        valid_idx_dict[class_code] = valid_id_class\n",
    "        test_idx_dict[class_code] = list(test_id_class)\n",
    "if RF:\n",
    "    train_id = sorted(train_id + valid_id)\n",
    "    test_id = sorted(test_id)\n",
    "\n",
    "    print(f'Train ratio: {len(train_id) / len(train_id+test_id):.2f}')\n",
    "    print(f'Test ratio: {len(test_id) / len(train_id+test_id):.2f}')\n",
    "else:\n",
    "    train_id = sorted(train_id)\n",
    "    valid_id = sorted(valid_id)\n",
    "    test_id = sorted(test_id)\n",
    "\n",
    "    print(f'Train ratio: {len(train_id) / len(train_id+valid_id+test_id):.2f}')\n",
    "    print(f'Validation ratio: {len(valid_id) / len(train_id+valid_id+test_id):.2f}')\n",
    "    print(f'Test ratio: {len(test_id) / len(train_id+valid_id+test_id):.2f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T10:46:35.501611600Z",
     "start_time": "2023-08-15T10:46:35.373583100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "train_id = np.array(train_id)\n",
    "train_real_only = list(train_id[train_id<1025])\n",
    "train_id = list(train_id)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T10:46:36.349801200Z",
     "start_time": "2023-08-15T10:46:36.231775100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Selection"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [1,2,5,6,7,8]\n",
    "crit_by_class = {0:0,\n",
    "                 1:0.055,\n",
    "                 2:0.1,\n",
    "                 3:0,\n",
    "                 4:0,\n",
    "                 5:0.05,\n",
    "                 6:0.3,\n",
    "                 7:0.3,\n",
    "                 8:0.01}\n",
    "crit_by_class_id = {}\n",
    "model = LSTMClassifier(input_dim=param.input_dim,\n",
    "                       n_features=param.n_features,\n",
    "                       window_size=1,\n",
    "                       latent_dim=param.hidden_dim,\n",
    "                       device=torch.device('cpu'),\n",
    "                       num_layers=param.num_layers)\n",
    "model.load_state_dict(torch.load(f'./cache/{name}_autoencoder.pth'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T10:42:52.970116100Z",
     "start_time": "2023-08-15T10:42:52.842087500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "fix_seed()\n",
    "# df_total = pd.concat([df_train, df_valid, df_test])\n",
    "r = real_instances.loc[train_real_only]\n",
    "for class_num in crit_by_class:\n",
    "    real_idx =list(r[r['class_code']==class_num].index.values)\n",
    "    sim_idx = list(sim_instances[sim_instances['class_code'] == class_num].index.values + 1025)\n",
    "    drawn_idx = list(drawn_instances[drawn_instances['class_code'] == class_num].index.values + 1964)\n",
    "    syn_idx = np.array(sim_idx + drawn_idx)\n",
    "    crit = crit_by_class[class_num]\n",
    "    syn_idx_fin = np.array(new_instances.loc[1025:][new_instances.loc[1025:, 'class_code']==class_num].index.values)\n",
    "\n",
    "    tot_idx = np.append(real_idx, syn_idx_fin)\n",
    "    df_total = make_data(df, tot_idx)\n",
    "    df_total = fit_scaler(tot_idx, df_total, clean_vars)\n",
    "\n",
    "    dataset = ByInstanceDataset(data=df_total, instance_id=tot_idx, all_instances=instances, is_binary=False,\n",
    "                      input_vars=clean_vars, scaler=None, stats=None, is_pretrain=True, already_scaled=False)\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    lst = {}\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(dataloader):\n",
    "            batch = batch.to(model.device)\n",
    "            hidden_tmp = model.encoder(batch)[0]\n",
    "            lst[idx] = hidden_tmp.detach().cpu().numpy().flatten()\n",
    "    d = pd.DataFrame(lst).T\n",
    "    pca = PCA(n_components=3)\n",
    "    pca.fit(d)\n",
    "    decomp_d = pca.fit_transform(d)\n",
    "\n",
    "    decomp_d_for_pdist = decomp_d\n",
    "    dist = squareform(pdist(decomp_d_for_pdist))[:len(real_idx),len(real_idx):]\n",
    "    mask = np.min(dist, axis=0) < crit\n",
    "    selected_id = syn_idx_fin[np.where(mask==True)]\n",
    "    crit_by_class_id[class_num] = list(selected_id)\n",
    "\n",
    "    decomp_d_real = decomp_d[:len(real_idx)]\n",
    "    decomp_d_sim = decomp_d[len(real_idx):len(real_idx) + len(sim_idx)]\n",
    "    decomp_d_drawn = decomp_d[len(real_idx) + len(sim_idx):]\n",
    "    decomp_d_syn = decomp_d[len(real_idx):]\n",
    "    decomp_d_aug = decomp_d[len(syn_idx):]\n",
    "\n",
    "    decomp_d_select = decomp_d_syn[mask]\n",
    "\n",
    "    plt.rcParams['figure.dpi'] = 300\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(decomp_d_select[:, 0], decomp_d_select[:, 1], decomp_d_select[:, 2],alpha=1,marker='*', s=100, c='green', label='Select')\n",
    "    ax.scatter(decomp_d_sim[:, 0], decomp_d_sim[:, 1], decomp_d_sim[:, 2],alpha=1, c='k', label='Simulate')\n",
    "    ax.scatter(decomp_d_drawn[:, 0], decomp_d_drawn[:, 1], decomp_d_drawn[:, 2],alpha=1, c='b', label='Drawn')\n",
    "    ax.scatter(decomp_d_aug[:, 0], decomp_d_aug[:, 1], decomp_d_aug[:, 2],alpha=0.1, c='grey', label='Augmentation')\n",
    "    ax.scatter(decomp_d_real[:, 0], decomp_d_real[:, 1], decomp_d_real[:, 2],alpha=1, c='r', label='Real')\n",
    "    plt.title(f'Problem {class_num}')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'./fig/problem_{class_num}_3d.png')\n",
    "    plt.close()\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(decomp_d_sim[:, 0], decomp_d_sim[:, 1], c='k', label='Simulate')\n",
    "    plt.scatter(decomp_d_drawn[:, 0], decomp_d_drawn[:, 1], c='b', label='Drawn')\n",
    "    plt.scatter(decomp_d_aug[:, 0], decomp_d_aug[:, 1],alpha=0.1, c='grey', label='Augmentation')\n",
    "    plt.scatter(decomp_d_select[:, 0], decomp_d_select[:, 1],marker='*', c='green',s=130, label='Select')\n",
    "    plt.scatter(decomp_d_real[:, 0], decomp_d_real[:, 1],  c='r', label='Real')\n",
    "    plt.title(f'Problem {class_num}')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'./fig/problem_{class_num}_x.png')\n",
    "    plt.close()\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(decomp_d_sim[:, 0], decomp_d_sim[:, 2], c='k', label='Simulate')\n",
    "    plt.scatter(decomp_d_drawn[:, 0], decomp_d_drawn[:, 2], c='b', label='Drawn')\n",
    "    plt.scatter(decomp_d_aug[:, 0], decomp_d_aug[:, 2],alpha=0.1, c='grey', label='Augmentation')\n",
    "    plt.scatter(decomp_d_select[:, 0], decomp_d_select[:, 2],marker='*', c='green',s=130, label='Select')\n",
    "    plt.scatter(decomp_d_real[:, 0], decomp_d_real[:, 2],  c='r', label='Real')\n",
    "    plt.title(f'Problem {class_num}')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'./fig/problem_{class_num}_y.png')\n",
    "    plt.close()\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(decomp_d_sim[:, 2], decomp_d_sim[:, 1], c='k', label='Simulate')\n",
    "    plt.scatter(decomp_d_drawn[:, 2], decomp_d_drawn[:, 1], c='b', label='Drawn')\n",
    "    plt.scatter(decomp_d_aug[:, 2], decomp_d_aug[:, 1],alpha=0.1, c='grey', label='Augmentation')\n",
    "    plt.scatter(decomp_d_select[:, 2], decomp_d_select[:, 1],marker='*', c='green',s=130, label='Select')\n",
    "    plt.scatter(decomp_d_real[:, 2], decomp_d_real[:, 1],  c='r', label='Real')\n",
    "    plt.title(f'Problem {class_num}')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'./fig/problem_{class_num}_z.png')\n",
    "    plt.close()\n",
    "\n",
    "    id_set = real_idx\n",
    "    max_v = df_total[clean_vars].max()\n",
    "    min_v = df_total[clean_vars].min()\n",
    "    plt.rcParams['figure.figsize'] = (8, 8)\n",
    "    fig, ax = plt.subplots(len(clean_vars), 1)\n",
    "    for id in id_set:\n",
    "        for idx, var in enumerate(clean_vars):\n",
    "            ax[idx].plot(range(len(df_total.loc[df_total['instance_id']==id])), df_total.loc[df_total['instance_id']==id, var])\n",
    "            ax[idx].set_title(var)\n",
    "            ax[idx].set_ylim([min_v[var], max_v[var]])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'./fig/problem_{class_num}_real.png')\n",
    "    plt.close()\n",
    "\n",
    "    id_set = selected_id\n",
    "    plt.rcParams['figure.figsize'] = (8, 8)\n",
    "    fig, ax = plt.subplots(len(clean_vars), 1)\n",
    "    for id in id_set:\n",
    "        for idx, var in enumerate(clean_vars):\n",
    "            ax[idx].plot(range(len(df_total.loc[df_total['instance_id']==id])), df_total.loc[df_total['instance_id']==id, var])\n",
    "            ax[idx].set_title(var)\n",
    "            ax[idx].set_ylim([min_v[var], max_v[var]])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'./fig/problem_{class_num}_selected.png')\n",
    "    plt.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T10:44:08.238835400Z",
     "start_time": "2023-08-15T10:42:53.582252900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in crit_by_class_id:\n",
    "    print(f'Problem{i}: {len(crit_by_class_id[i])}')\n",
    "crit_by_class_id[0] = []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Train LSTM Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ratio: 0.63\n",
      "Validation ratio: 0.22\n",
      "Test ratio: 0.16\n"
     ]
    }
   ],
   "source": [
    "param.num_epoch = 100\n",
    "param.batch_size = 64\n",
    "param.learning_rate = 0.003\n",
    "by_instance = True\n",
    "use_synthetic = False\n",
    "use_augmentation = False\n",
    "use_selected = True\n",
    "integrate = False\n",
    "integrate_syn = False\n",
    "# Real_Only | With_Sim_Drawn | With_Sim_Drawn_Augmentation | Byinstance_Add_selected\n",
    "# 첫번째 Each => scaler를 real이랑 synthetic이랑 분리\n",
    "# 두번째 Each => scaler를 synthetic 내에서 simulatd랑 drawn이랑 분리\n",
    "# Byinstance => instance마다 스케일링\n",
    "fix_seed()\n",
    "\n",
    "\n",
    "for class_code in set(real_instances['class_code']):\n",
    "        if use_selected and (len(crit_by_class_id[class_code]) != 0):\n",
    "            train_valid_id = crit_by_class_id[class_code]\n",
    "            train_id_class, valid_id_class = train_test_split(train_valid_id, test_size=args.validation_ratio/(1-args.test_ratio))\n",
    "            train_id += train_id_class\n",
    "            valid_id += valid_id_class\n",
    "\n",
    "            train_idx_dict[class_code] += train_id_class\n",
    "            valid_idx_dict[class_code] += valid_id_class\n",
    "\n",
    "if RF:\n",
    "    train_id = sorted(train_id + valid_id)\n",
    "    test_id = sorted(test_id)\n",
    "\n",
    "    print(f'Train ratio: {len(train_id) / len(train_id+test_id):.2f}')\n",
    "    print(f'Test ratio: {len(test_id) / len(train_id+test_id):.2f}')\n",
    "else:\n",
    "    train_id = sorted(train_id)\n",
    "    valid_id = sorted(valid_id)\n",
    "    test_id = sorted(test_id)\n",
    "\n",
    "    print(f'Train ratio: {len(train_id) / len(train_id+valid_id+test_id):.2f}')\n",
    "    print(f'Validation ratio: {len(valid_id) / len(train_id+valid_id+test_id):.2f}')\n",
    "    print(f'Test ratio: {len(test_id) / len(train_id+valid_id+test_id):.2f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T10:46:42.506209800Z",
     "start_time": "2023-08-15T10:46:42.378181400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "df_train = make_data(df, train_id)\n",
    "df_valid = make_data(df, valid_id)\n",
    "df_test = make_data(df, test_id)\n",
    "\n",
    "if by_instance:\n",
    "    df_train = fit_scaler(train_id, df_train, clean_vars)\n",
    "    df_valid = fit_scaler(valid_id, df_valid, clean_vars)\n",
    "    df_test = fit_scaler(test_id, df_test, clean_vars)\n",
    "\n",
    "else:\n",
    "    if integrate:\n",
    "        scaler_train = make_scaler(train_id, df_train, clean_vars)\n",
    "        df_train = fit_scaler_old(train_id, df_train, clean_vars, scaler_train)\n",
    "        df_valid = fit_scaler_old(valid_id, df_valid, clean_vars, scaler_train)\n",
    "        df_test = fit_scaler_old(test_id, df_test, clean_vars, scaler_train)\n",
    "    else:\n",
    "        train_id = np.array(train_id)\n",
    "        train_id_real = train_id[train_id < 1025]\n",
    "        scaler_train_real = make_scaler(train_id_real, df_train, clean_vars)\n",
    "        df_train = fit_scaler_old(train_id_real, df_train, clean_vars, scaler_train_real)\n",
    "        df_valid = fit_scaler_old(valid_id, df_valid, clean_vars, scaler_train_real)\n",
    "        df_test = fit_scaler_old(test_id, df_test, clean_vars, scaler_train_real)\n",
    "\n",
    "        if integrate_syn:\n",
    "            train_id_syn = train_id[train_id >= 1025]\n",
    "            valid_id_syn = valid_id[valid_id >= 1025]\n",
    "            scaler_train_syn = make_scaler(train_id_syn, df_train, clean_vars)\n",
    "            df_train = fit_scaler_old(train_id_syn, df_train, clean_vars, scaler_train_syn)\n",
    "            df_train = fit_scaler_old(valid_id_syn, df_train, clean_vars, scaler_train_syn)\n",
    "        else:\n",
    "            train_id_sim = set(df_train.loc[df_train['source']=='simulated', 'instance_id'])\n",
    "            train_id_drawn = set(df_train.loc[df_train['source']=='drawn', 'instance_id'])\n",
    "            valid_id_sim = set(df_valid.loc[df_valid['source']=='simulated', 'instance_id'])\n",
    "            valid_id_drawn = set(df_valid.loc[df_valid['source']=='drawn', 'instance_id'])\n",
    "\n",
    "            scaler_train_sim = make_scaler(train_id_sim, df_train, clean_vars)\n",
    "            scaler_train_drawn = make_scaler(train_id_drawn, df_train, clean_vars)\n",
    "\n",
    "            df_train = fit_scaler_old(train_id_sim, df_train, clean_vars, scaler_train_sim)\n",
    "            df_train = fit_scaler_old(train_id_drawn, df_train, clean_vars, scaler_train_drawn)\n",
    "            df_valid = fit_scaler_old(valid_id_sim, df_valid, clean_vars, scaler_train_sim)\n",
    "            df_valid = fit_scaler_old(valid_id_drawn, df_valid, clean_vars, scaler_train_drawn)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T10:47:18.661024200Z",
     "start_time": "2023-08-15T10:47:14.475073400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "for class_num in range(9):\n",
    "    train_idx_dict[class_num]=sorted(train_idx_dict[class_num])\n",
    "    valid_idx_dict[class_num]=sorted(valid_idx_dict[class_num])\n",
    "    test_idx_dict[class_num]=sorted(test_idx_dict[class_num])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T10:47:22.183362Z",
     "start_time": "2023-08-15T10:47:22.060311400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for class_num in [1,2,5,6,7,8]:\n",
    "    length = 0\n",
    "    length_min = 1e8\n",
    "    for i in train_idx_dict[class_num]:\n",
    "        len_tmp = len(df_train[df_train['instance_id'] == i])\n",
    "        length += len_tmp\n",
    "        length_min = min(length_min, len_tmp)\n",
    "    length /= len(train_idx_dict[class_num])\n",
    "\n",
    "    window_size =  min(int(length/30), int(length_min/15))\n",
    "    stride = max(int(window_size / 3),1)\n",
    "\n",
    "    if class_num ==1:\n",
    "        window_size = 15\n",
    "        stride = 3\n",
    "    if class_num ==2:\n",
    "        window_size = 15\n",
    "        stride = 3\n",
    "    if class_num == 8:\n",
    "        window_size = 4\n",
    "        stride = 2\n",
    "\n",
    "    train_dataset = SlidingWindowDataset(data=df_train, instance_id=train_idx_dict[class_num], window_size=window_size, stride=stride,\n",
    "                                                 input_vars=clean_vars, already_scaled=True)\n",
    "    valid_dataset = SlidingWindowDataset(data=df_valid, instance_id=valid_idx_dict[class_num], window_size=window_size, stride=stride,\n",
    "                                                 input_vars=clean_vars, already_scaled=True)\n",
    "    test_dataset = SlidingWindowDataset(data=df_test, instance_id=test_idx_dict[class_num], window_size=window_size, stride=stride,\n",
    "                                                 input_vars=clean_vars, already_scaled=True)\n",
    "    dataloader_dict = {}\n",
    "    dataloader_dict['Train'] = DataLoader(train_dataset, batch_size=param.batch_size, shuffle=False)\n",
    "    dataloader_dict['Valid'] = DataLoader(valid_dataset, batch_size=param.batch_size, shuffle=False)\n",
    "    dataloader_dict['Test'] = DataLoader(test_dataset, batch_size=param.batch_size, shuffle=False)\n",
    "\n",
    "    fix_seed()\n",
    "    model = LSTMClassifier(input_dim=param.input_dim,\n",
    "                           n_features=param.n_features,\n",
    "                           window_size=window_size,\n",
    "                           latent_dim=param.hidden_dim,\n",
    "                           device=param.device,\n",
    "                           num_layers=param.num_layers)\n",
    "\n",
    "    model.load_state_dict(torch.load(f'./cache/{name}_autoencoder.pth'))\n",
    "    input_size = model.classifier.input_size\n",
    "    model.classifier.fc[-1] = nn.Linear(int(input_size/4), 2)\n",
    "\n",
    "    for p in model.reconstruct_decoder.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in model.encoder.parameters():\n",
    "        p.requires_grad = True\n",
    "    params_to_optimize = [{'params': model.encoder.parameters()}, {'params': model.classifier.parameters()}]\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(params_to_optimize, lr=param.learning_rate)\n",
    "    #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.96)\n",
    "\n",
    "    fix_seed()\n",
    "    model, logger = Train_Classifier(param, model, criterion, optimizer, scheduler, dataloader_dict, model_name=f'{name}_binary_class{class_num}')\n",
    "\n",
    "    draw_loss(logger, view='loss',fname=f'./Mission2/{name}_binary_class{class_num}_ws{window_size}_stride{stride}_loss')\n",
    "    draw_loss(logger, view='acc',fname=f'./Mission2/{name}_binary_class{class_num}_ws{window_size}_stride{stride}_acc')\n",
    "    prediction, real = Inference_Classifier(model, dataloader_dict['Test'], criterion=criterion)\n",
    "\n",
    "    for inst in test_idx_dict[class_num]:\n",
    "        tmp_dataset = SlidingWindowDataset(data=df_test, instance_id=[inst], window_size=window_size, stride=stride,\n",
    "                                                     input_vars=clean_vars, already_scaled=True)\n",
    "        tmp_dataloader = DataLoader(tmp_dataset, batch_size=1, shuffle=False)\n",
    "        prediction, real = Inference_Classifier(model, tmp_dataloader, criterion=criterion)\n",
    "\n",
    "        _, pred_labels = torch.max(prediction, dim=1)\n",
    "        _, true_labels = torch.max(real, dim=1)\n",
    "        pred_labels = pred_labels.detach().cpu()\n",
    "        true_labels = true_labels.detach().cpu()\n",
    "\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(true_labels.numpy(), pred_labels.numpy(), average='micro')\n",
    "        df_tmp = df_test[df_test['instance_id']==inst].iloc[window_size::stride].reset_index(drop=True)\n",
    "        df_tmp.loc[:, 'timestamp'] = pd.to_datetime(df_tmp.loc[:, 'timestamp'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "        time = df_tmp.loc[:, 'timestamp']\n",
    "        # df_tmp.loc[:, clean_vars] = scaler_train.inverse_transform(df_tmp.loc[:, clean_vars])\n",
    "        plt.rcParams['figure.figsize'] = (8, 8)\n",
    "        fig, ax = plt.subplots(len(clean_vars), 1)\n",
    "\n",
    "        label = {'P-TPT':'Pressure, Pa',\n",
    "                 'P-MON-CKP': 'Pressure, Pa',\n",
    "                 'T-TPT': 'Temperature, \\u00B0C'}\n",
    "        df_tmp_realscale = df[df['instance_id']==inst].iloc[window_size::stride].reset_index(drop=True)\n",
    "        for idx, var in enumerate(clean_vars):\n",
    "            ax[idx].plot(time, df_tmp_realscale.loc[:, var].reset_index(drop=True), c='k')\n",
    "            ax[idx].set_title(var)\n",
    "            ax[idx].axvline(time[np.where(true_labels==1)[0][0]],c=[0, 0, 0.8],linestyle='--', label='Event Period')\n",
    "            ax[idx].axvline(time[np.where(pred_labels==1)[0][0]],c=[0.8, 0, 0],linestyle='-', label='Pred Period')\n",
    "\n",
    "            ax[idx].legend()\n",
    "            ax[idx].axvspan(time[0], time[np.where(pred_labels==1)[0][0]], facecolor='green', alpha=0.2)\n",
    "            ax[idx].axvspan(time[np.where(pred_labels==1)[0][0]], time[len(df_tmp)-1], facecolor='r', alpha=0.2)\n",
    "            ax[idx].set_xlim([time[0], time[len(df_tmp)-1]])\n",
    "            ax[idx].set_ylabel(label[var])\n",
    "            ax[idx].set_xlabel('Time, Minutes')\n",
    "            ax[idx].xaxis.set_major_formatter(matplotlib.dates.DateFormatter('%b %d\\n%H:%M'))\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # plt.plot(true_labels.detach().cpu(), label='True')\n",
    "        # plt.plot(pred_labels.detach().cpu(), label='Prediction')\n",
    "        plt.legend()\n",
    "        plt.savefig(f'./fig/Mission2/{class_num}/{name}_ws{window_size}_stride{stride}_{inst}_f1_{np.average(f1):.3f}.png')\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Sumário",
   "title_sidebar": "Sumário",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
